Required Steps for Training:

(* for training using BERT model  requires about 4 GPU, e.g. GTX1080ti (40 Gig RAM))

1- Training the encoder h     
 (* we observed moderate variations in AUROC score for different training runs of h. We trained h 3 times and report the evaluation with best h.
  for AG-NEWS datasets you find the checkpoint for the best trained h in the trained_model directory under 
         1-AG_NEWS_h_net_.pt   


   -python train_encoder.py  

   
notice that:
  1- checkpoint is saved under ./checkpoint/AG_NEWS_hdim_{args.h_dim}_h_net.pt 
  2- hyperparameters can be changed and passed as arguments 
   
   
2- Training the discriminator s 
 (* which requires at least 4 GPUs)   
 (* most of the variation in AUROC comes from variation in s for different training runs
 
   -python train_discriminator.py 


  
  
3- Evaluating the performance of the discriminator s
    (* requires at least 4 GPUs)
    
    - python evaluate_discriminator.py --h_net_path=<path to the trained h_net>  --s_net_path=<path to the trained s_net>
  

  
